1. 神经网络中包含哪些超参数？

网路参数:卷积核数量，尺寸，网络深度，激活函数

优化参数:学习率，batch size

正则参数:dropout，权重衰减参数

2. CRF
    CRF学习实体标签之间的约束规则保证标签的合法性。无向图
    条件：给定观测状态的先验条件。（通过相邻位置状态（马尔可夫随机场）和观测状态判断当前位置状态）
    CRF的损失函数是求最优路径的概率占所有路径概率和的比例，目标是最大化整个比例。
    暴力解法求出所有标签排列的概率找最大，当然不可取，CRF采用维特比算法求解。

3. 维特比算法
    是一种动态规划算法，用来求解最有可能产生观测序列的维特比路径（横轴观测序列（文字序列），纵轴隐状态（BIO标签））
在每一时刻，计算当前时刻落在每种隐状态的最大概率，并记录这个最大概率从哪一个隐状态转移过来，最后再从结尾回溯最大概率，得到最有可能的最优路径
    实现：两个暂存表格$T1，T2$
    $T1[i,j]$表示$j$时刻落到隐状态$q_i$的最大概率，$T2[i,j]$表示最大概率从第$j-1$时刻的哪种隐状态$q_i$转移过来的，第0时刻$T2$为$NAN$，乘法取对数变加法
    时间复杂度$O(TN^2)$
    
4. HMM
    是概率图模型的一种，属于生成模型，描述由隐状态序列生成可观测状态的过程。

    有向无环图。预测天气的温度，温度变化是隐状态，雪糕销量是观测状态，通过转移概率矩阵和观测概率矩阵预测是为HMM模型，是为生成模型
    直接通过前一天的温度预测当前温度，是为判别模型P(Y|X)

    -------

    模型参数：$\lambda = (\pi,A,B)$

    观测序列：$O = (o_1,o_2,...,o_T)$

    隐状态序列：$I = (i_1,i_2,...,i_T)$

    - 三个基本问题：概率计算（已知模型和观测序列，计算该模型下观测序列出现的概率），参数学习（已知观测序列，估计模型的参数，（不包含对应的状态序列，采用非监督学习算法，EM算法，包含对应的状态序列，监督学习，极大似然估计），解码问题(已知模型参数和观测序列，求最大概率的隐状态序列，维特比算法)
    - 两个假设：一阶马尔科夫假设，当前状态与前一时刻状态有关；观测独立假设，当前观测与当前时刻状态有关。

    - HMM参数学习：极大似然估计（统计思想，极大频率即概率）进行监督学习

      1）初始状态概率的估计

      ​	<font size=5>$\hat \pi_{q_i} = \frac{count(q_i^1)}{count(o_1)}$  </font>                                                       <font size=4> $\sum_i\pi_{q_i}=1$</font>

      2）转移概率的估计

      <font size=4>$\hat A_{ij} = P(i_{t+1} = q_j|i_t = q_i) =  \frac{count(q_iq_j)}{count(q_i)}$  </font>                   <font size=4> $\sum_j A_{ij}=\sum_j P(i_{t+1} = q_j|i_t = q_i) = 1$</font>

      3）发射概率的估计

      <font size=4>$\hat B_{jk} = P(o_t = v_k|i_t = q_j) =  \frac{count(q_jv_k)}{count(q_j)}$  </font>                              <font size=4> $\sum_k B_{jk}=\sum_k  P(o_t = v_k|i_t = q_j) = 1$</font>
      ---

    - 预测用HMM解决序列标注问题

      ![image-20210118171248118](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210118171248118.png)

    

5. 判别模型VS生成模型
    判别模型：对P(Y|X)建模，根据输入的特征，预测标签，计算损失，学习参数，如LR，CRF，速度快
    生成模型：对联合概率P(X,Y)建模，P(X,Y) = P(X|Y)P(Y)，P(Y|X) = P(X,Y)/P(X)，速度慢

6. Attention
    表示重要性的权重向量

7. Seq2Seq
    任意长度源序列转化为目标序列任务，如常见的机器翻译和语音识别
    经典架构模型：Encoder-Decoder

    - RNN Encoder-Decoder
      核心特征在最后一层
      ![image-20210124163535576](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210124163535576.png)
    
      将变长源序列X编码成定长向量c，并将c解码成变成变长目标序列Y。
      编码器和解码器联合训练，以最大化给定源序列的目标序列的条件概率。
    
    - Attention Encoder-Decoder
      ![image-20210124165337815](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210124165337815.png)
    
      RNN 定长C表示能力有限（只在最后一刻），因此提出Attention，对每一时刻的加权隐藏特征解码
    
      权重理解为系数（先计算数值再softmax），计算打分函数方法多种多样。
    
    - Attention 
    
      - soft Attention
        ![image-20210124174642131](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210124174642131.png)
    
        - 在输入信息上计算注意力分布
          根据注意力分布计算输入信息的加权平均(softmax)
          $\alpha_i = softmax(s(x_i,q))$ （先和q计算打分，又和它进行加权平均）
          $\alpha_i$称为注意力分布（可以理解为选中的概率，也可以理解为提取的信息占比）
          $s(x_i,q)$成为注意力打分函数($x_i$可理解为上图中的$h_i$，q可理解为上图中的$s_{t-1}$)
          打分函数计算方式：
        - 求和模型：$s(x_i,q)= V^T tanh(Wx_i  \ + Uq)$ 
        - 缩放点积模型：$s(x_i,q)= \frac{x_i^T q}{\sqrt d}$    (d为输入信息的维度，序列长度，为了模型更加稳定，序列较长时，点积运算方差较大（数值偏离程度较大，比如100,0,-100的方差比1,0,-1大），进行softmax运算(指数次方)信息会集中在大数值上，造成梯度较小，缩放点积可以降低方差（类似于归一化）)，点积可以利用矩阵乘积，计算效率高
    
      - 键值对注意力机制    
        ![image-20210124174608062](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210124174608062.png)
        
      - 自注意力机制 
      $Q = W_Q X$  查询向量序列
      $K = W_K X$  键向量序列
      $V = W_V X$  值向量序列
        
      
      $W_Q,W_K,W_V $为可学习参数矩阵
      
      
    
8. transformer
   
    注意力机制丢失了位置信息(单词的前后关系)
    
    优点：并行性和长距离特征依赖
    
    输入序列词嵌入Embedding , 维度(N*d）
    
    输入序列位置编码pos_emd ,维度(N*d)  (位置序号使用偶数位置sin和奇数位置cos函数)
    
    输入序列：序列序号
    目标序列：decode最上面那个$[t_1,t_2,···，t_q，t_{q+1},t_n]$
    
    outputs序列:decode最下面那个[$t_1,t_2,···,t_{q-1}$]
    
     $Z=Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}}) V$
    
    ![image-20210124183114335](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210124183114335.png)
    
    多头机制(multi-headed):不同的(k,q,v)组计算得到的z通过拼接再降维得到多个特征表达，作用类似于卷积核数量
    
9.  bert

    - Transformer作为特征抽取器
    - 语言模型作为训练任务（双向）
    
    两阶段模型：
    
    1是语言模型预训练
    
    2是使用fine-tuning解决下游任务
    
    ![image-20210124185733963](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210124185733963.png)
    
    
    
11. ELMO
     ELMO：Embedding from Language Models
     根据当前上下文对Word Embedding动态调整![image-20210126143024991](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210126143024991.png)

     特征抽取器采用的是RNN

12. GPT
     ![image-20210126144110277](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210126144110277.png)

     - 特征抽取器采用transformer
     - 单向语言模型
    
13. CNN
    

池化：降采样，最大池化层，取最大特征，提取纹理    
MaxPool1d(kernel_size）   
![image-20210206152610220](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210206152610220.png)    
小卷积核作用：降低参数量（卷积核权值共享）

13. 矩阵乘法导数 

     $X:[N,D]，W:[D,H]，Y:[N,H]$ 

     $Y = XW$

     $\frac{\sigma L}{\sigma x} = \frac{\sigma L}{\sigma y} W^T，\frac{\sigma L}{\sigma w} = x^T\frac{\sigma L}{\sigma y}$

     

14. Word2vec

- 静态向量，一词多义没有解决

  - cbow（完形填空，上下文预测输出），skip-gram（用词造句，词预测上下文）
    训练采用负采样（输入和正确输出全部当做输入，1做标签，输入和非上下文做0标签，转化为二分类问题）

  - 多分类到二分类思想：选择题变判断题（充分利用先验，已知正确输出）

    ![image-20210203135701279](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210203135701279.png)

    ![image-20210203135836145](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210203135836145.png)

  - 负采样：

    ![image-20210203140335431](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210203140335431.png)
16. RNN
	
	​                                                      ![image-20210203152331238](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210203152331238.png)
	
	![image-20210203153848270](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210203153848270.png)
	
17. 激活函数
    
    + $tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}  ,y = tanh(x),\frac{\partial y}{\partial x} = 1 - y^2  $
    + $y = \frac {1}{1+e^{-x}},\frac{\partial y}{\partial x} = y(1-y) $
    
18. 数据增强

    - 词语替换
      - 垂直领域同义词替换
      - word2vec近义词替换
    - 翻译再翻译（重要句子）

19. 深度学习
    why：从训练数据自动获取最优权重参数
    how:引入损失函数（关于权重的函数）指标，使它达到最小，优化算法：利用函数斜率的梯度法

20. 模型蒸馏

    - ALBERT

      - Embding矩阵分解：减小词嵌入的参数量（$V*H = V*E+E*H$）

        attention层和FFN层参数共享
        预训练NSP改进

    - TinyBert
      loss = mse(embdding)+mse(attention+hidden)+crossent(pred)

      ![image-20210206135339178](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210206135339178.png)![image-20210206135408982](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210206135408982.png)

      ![image-20210206135449538](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210206135449538.png)

      ![image-20210206135504661](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210206135504661.png)
      ![image-20210206135520727](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210206135520727.png)

21. 交叉熵：

**似然性**用于根据一些观测结果，估计给定模型的参数可能值。

假设$X$是观测结果序列，它的概率分布$f_{x}$依赖于参数$ \theta$，则似然函数表示为

$L(\theta|x) = P_\theta(X=x)$

交叉熵：表示实际预测概率分布与真实概率分布的距离（相似度），值越小表示越相似，也就是寻找低熵的模型参数（熵越低概率越确定，0熵即真实概率）

通常定义：$L = -\sum_i{p_ilog(q_i)+(1-p_i)log{(1-q_i)}}$,例如

$y = [1,0,0]$

$\hat y = [0.6,0.3,0.1]$

$L = -(1log0.6+0*log0.4+0log0.3+1*log0.7+0*log0.1+1*log0.9)=-(log0.6+log0.7+log0.9)$

pytorch中交叉熵损失：

有$N$个样本，输入一个$C$分类器，得到的输出为$X∈R^{N×C}$,

$ \text{loss}(x, class) = -\frac{1}{N}\sum_i^N \log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)$

$L_{pytorch}=log\left[e^{0.6},e^{0.3},e^{0.1}\right]\left(e^{0.6}+e^{0.3}+e^{0.1}\right)[0]$ [0]表示真实标签1所在的索引位置

``` python
input = torch.randn(3, 5)
target = torch.tensor([1, 0, 4])
F.cross_entropy(input,target)#等价
F.nll_loss(F.log_softmax(input,dim=1),target) #等价nll为negtive log likehood
F.nll_loss(torch.log(F.softmax(input,dim=1)),target) #等价
-torch.mean(torch.log(F.softmax(input,dim=1)[torch.arange(target.shape[0]),target]))#等价
-torch.mean(torch.log(F.softmax(input-torch.max(input,dim=1,keepdim=True)[0],dim=1)[torch.arange(target.shape[0]),target]))#等价
```

22.样本不均衡

​	样本重新分配权重->损失重新分配权重（查缺补漏）

- 多数类和少数类（类别数量不均衡）：引入带权重的交叉熵损失，$W∈R^{1×C}$，那么损失

  $ \text{loss}(x, class) = -\frac{1}{N}\sum_i^N W\left[class\right]\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)$

- 难分类样本和易分类样本（特征不容易区分）：引入Focal Loss损失,($\gamma$越大，越抑制容易分类损失，方大难分类损失)
  $p = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)$
  $ \text{loss}(x, class) = -weight[class]\left(1 - p\right)^\gamma log(p) = -w(1-p)^\gamma log(p) $

  ```python
  class FocalLoss(nn.Module):
      def __init__(self, gamma=2, weight=0.25, reduction='mean'):
          super(FocalLoss, self).__init__()
          self.gamma = gamma
          self.weight = weight
          self.reduction = reduction
   
      def forward(self, output, target):
          # convert output to pseudo probability
          out_target = output[torch.arange(target.shape[0]),target]
          probs = torch.sigmoid(out_target)
          focal_weight = torch.pow(1-probs, self.gamma) 
          # add focal weight to cross entropy
          ce_loss = F.cross_entropy(output, target, weight=self.weight, reduction='none')
          focal_loss = focal_weight * ce_loss
          if self.reduction == 'mean':
              focal_loss = (focal_loss/focal_weight.sum()).sum()
          elif self.reduction == 'sum':
              focal_loss = focal_loss.sum() 
          return focal_loss
  ```

  

23 多标签分类

$\log \left(1 + \sum\limits_{i\in\Omega_{neg}} e^{s_i}\right) + \log \left(1 + \sum\limits_{j\in\Omega_{pos}} e^{-s_j}\right) $

